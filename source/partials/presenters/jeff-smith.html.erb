<section class="presentation" id="jeff-smith">
    <div class="presentation-presenter">
        <div class="presentation-presenter__avatar-container">
            <img src="/assets/images/people/presenters/jeff-smith.jpg" class="presentation-presenter__avatar" alt="Jeff Smith" />
        </div>

        <h4 class="presentation-presenter__name">Jeff Smith</h4>

        <p class="presentation-presenter__link"><a href="https://twitter.com/jeffksmithjr">@jeffksmithjr</a></p>
    </div>

    <div class="presentation__details">
        <h3 class="presentation__title">Neuroevolution in Elixir</h3>
        <div class="presentation__description">
          <p>
          Python has become the dominant language for machine learning, and as an Elixir dev, that’s a real bummer. When building machine learning systems, there’s lots of reasons to want to use tools like metaprogramming, concurrency, supervision, distribution, higher-order functions, and all of the other things that Elixir does well. This talk introduces a library that tries to let Elixir and Python each do what they do best, in collaboration.
          </p>
          <p>
          Galápagos Nǎo is an Elixir library to evolve deep neural networks. Under the hood it uses the Gluon Python API to Apache MXNet, to take advantage of one of the most flexible and performant deep learning libraries around. The goal of this unholy union of technologies is to do something even harder than building deep learning models: build an ecosystem that evolves deep learning model architectures. Just for fun, the framework also includes shiny features like interactive evolution and optimization of architectures for non-functional requirements.
          </p>
        </div>
        <div class="presentation__bio">
            <p><strong>About Speaker: </strong>Jeff Smith is an AI developer, author, and manager. He coined the term reactive machine learning and wrote the definitive text on the topic.</p>
        </div>
    </div>
</section>

